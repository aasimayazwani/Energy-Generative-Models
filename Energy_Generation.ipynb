{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2806a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aasimwani/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020c37d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5y/b7t5j5vs6zd0r623lr2ygmwm0000gn/T/ipykernel_6384/858126115.py:421: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data =  data.append(df2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains  161373  rows and  35  columns.\n",
      "\n",
      "\n",
      "The extra unncessary columns have been deleted\n",
      "The dataset now contains  161373  rows and  11  columns.\n",
      "\n",
      "\n",
      "The time related features extracted for the start of the trip\n",
      "The dataset now contains  161373  rows and  14  columns.\n",
      "\n",
      "\n",
      "The dataset now contains  161320  rows and  12  columns.\n",
      "The Null Values have been dropped and index resest with extra column deleted\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import math\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "import holidays \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px \n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "class Feature_Extraction():\n",
    "    \n",
    "    def GET_deleted_columns(self,data,variables_to_remove):\n",
    "        \"\"\"\n",
    "        This functions is used to maintain the list of variables which are highly collinear with the \n",
    "        output variable and must be removed to maintain a good list of training data\n",
    "        \"\"\"\n",
    "        for variable in variables_to_remove:\n",
    "            try:\n",
    "                data = data.drop(columns = [variable])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return data\n",
    "\n",
    "    def GET_Scaled_data(self,data):\n",
    "        column_names = data.columns\n",
    "        scaler = StandardScaler()\n",
    "        #print(scaler.fit(data))\n",
    "        scaled_data = scaler.fit_transform(data)\n",
    "        scaled_data = pd.DataFrame(scaled_data)\n",
    "        scaled_data.columns = column_names\n",
    "        return scaled_data, scaler\n",
    "    \n",
    "    def GET_data_splits(self,X,Y):\n",
    "        \"\"\"\n",
    "        The objective of this function is to split the data into it's own train/test/validation splits\n",
    "        X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, shuffle = True)\n",
    "        \n",
    "        X_train = X_train.reset_index(inplace = False)\n",
    "        X_train = X_train.drop(columns = [\"index\"])\n",
    "        \n",
    "        X_test = X_test.reset_index(inplace = False)\n",
    "        X_test = X_test.drop(columns = [\"index\"])\n",
    "        \n",
    "        y_train = y_train.reset_index(inplace = False)\n",
    "        y_train = y_train.drop(columns = [\"index\"])\n",
    "        \n",
    "        y_test = y_test.reset_index(inplace = False)\n",
    "        y_test = y_test.drop(columns = [\"index\"])\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def GET_reduce_time_features(self,df,variable_name):\n",
    "        df[variable_name] = pd.to_datetime(df[variable_name],unit='s')\n",
    "        list_of_times = []\n",
    "        for i in range(0,len(df)):\n",
    "            date = str(df[variable_name][i])\n",
    "            datem = datetime.datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "            extracted_time = [datem.year,datem.month,datem.day,datem.hour]\n",
    "            list_of_times.append(extracted_time)\n",
    "        combined = pd.DataFrame(list_of_times)\n",
    "        combined.columns = [variable_name+\"_year\",\n",
    "                           variable_name+\"_month\",\n",
    "                           variable_name+\"_day\",\n",
    "                           variable_name+\"_hour\"]\n",
    "        merged_df = pd.concat([df, combined], axis=1, join='inner')\n",
    "        merged_df = merged_df.drop(columns = [variable_name])\n",
    "        return merged_df\n",
    "    \n",
    "    def GET_time_features(self,data):\n",
    "        data = self.SET_day_type(data,\"start_time\")\n",
    "        data = self.GET_reduce_time_features(data,\"start_time\")\n",
    "        data = self.GET_reduce_time_features(data,\"end_time\")\n",
    "        return data \n",
    "    \n",
    "    def SET_day_type(self,data,variable_name):\n",
    "        # Weekdays = 0 \n",
    "        # Weekend = 1\n",
    "        # Holiday = 2\n",
    "        data[\"temp\"] = pd.to_datetime(data[variable_name],unit='s')\n",
    "        result = []\n",
    "        import holidays\n",
    "        us_holidays = holidays.US()\n",
    "        for i in range(0,len(data)):\n",
    "            current_time = data[\"temp\"][i]\n",
    "            if current_time in us_holidays:\n",
    "                result.append(2)\n",
    "            else:\n",
    "                day_type = current_time.weekday()\n",
    "                if day_type <= 5:\n",
    "                    result.append(0)\n",
    "                if day_type >5:\n",
    "                    result.append(1)\n",
    "        data[\"day_type\"] = result\n",
    "        data = data.drop(columns = [\"temp\"])\n",
    "        return data\n",
    "    \n",
    "    def GET_X_and_Y(self,data,target_variable):\n",
    "        X = data[[item for item in data.columns if item != target_variable]]\n",
    "        Y = data[[target_variable]]\n",
    "        return X, Y\n",
    "    \n",
    "    def GET_BUS_DESIGN(self,data):    \n",
    "        manufacturer = []\n",
    "        capacity = []\n",
    "        missing = 0 \n",
    "        for i in range(0,len(data)):\n",
    "            current = data[\"bus_id\"][i]\n",
    "            #print(\"fault\")\n",
    "            # 1 - Corresponds to New Flyer -> culver city \n",
    "            # 2 - Corresponds to Portera  - > foothill \n",
    "            # 3 - Corresponds to Alexander Dennis\n",
    "            if str(current)[0:1] == \"7\":\n",
    "                manufacturer.append(1)\n",
    "                if current == 7157:\n",
    "                    capacity.append(350)\n",
    "                else:\n",
    "                    # this includes 7156, 7158, 7159\n",
    "                    capacity.append(440)\n",
    "            if str(current)[0:1] in \"2\":\n",
    "                manufacturer.append(2)\n",
    "                if current in list(range(2001,2016)):\n",
    "                    capacity.append(72)\n",
    "                if current in list(range(2016,2018)):\n",
    "                    capacity.append(79)\n",
    "                if current in list(range(2600,2614)):\n",
    "                    capacity.append(440)\n",
    "                if current in list(range(2800,2803)):\n",
    "                    capacity.append(440)\n",
    "\n",
    "            if str(current)[0:2] == \"30\":\n",
    "                manufacturer.append(3)\n",
    "                if current in [3000,3001]:\n",
    "                    capacity.append(648)\n",
    "\n",
    "        if len(capacity) != len(data) or  len(manufacturer) != len(data) :\n",
    "            print(\"UNIDENTIFIED BUS_ID ENTERED\")\n",
    "\n",
    "        else:\n",
    "            data[\"BATTERY_CAPACITY\"] = capacity\n",
    "            data['MANUFACTURER'] = manufacturer\n",
    "            data = data.drop(columns = [\"bus_id\"])\n",
    "            return data\n",
    "        \n",
    "class CLUSTERING_MODEL(): \n",
    "    def __init__(self,num_iterations,num_clusters):\n",
    "        self.iterations = num_iterations\n",
    "        self.clusters = num_clusters\n",
    "        self.data = data.copy()\n",
    "\n",
    "    def gmm(self):\n",
    "        from sklearn.mixture import GaussianMixture\n",
    "        total_predictons = []\n",
    "        for i in range(0,self.iterations):\n",
    "            gm = GaussianMixture(n_components=self.clusters , random_state=0).fit_predict(self.data)\n",
    "            total_predictons.append(pd.DataFrame(gm).T)\n",
    "        concatenated = pd.concat(total_predictons)\n",
    "        round_kmeans_predictions = round(concatenated.mean(axis=0))\n",
    "        self.data[\"GMM_Prediction\"]=round_kmeans_predictions\n",
    "        self.data['GMM_Prediction']=self.data['GMM_Prediction'].replace(0,-1)\n",
    "\n",
    "    def agglomerative(self):\n",
    "        num_iterations = self.iterations/10\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        total_predictons = []\n",
    "        for i in range(0,num_iterations):\n",
    "            prediction = AgglomerativeClustering().fit_predict(self.data)\n",
    "            total_predictons.append(pd.DataFrame(prediction).T)\n",
    "        concatenated = pd.concat(total_predictons)\n",
    "        agglomerative_predictions = round(concatenated.mean(axis=0))\n",
    "        self.data[\"Agglomerative\"]=agglomerative_predictions\n",
    "        self.data['Agglomerative']=self.data['Agglomerative'].replace(0,-1)\n",
    "        return data\n",
    "\n",
    "    def kmeans(self):\n",
    "        num_iterations = self.iterations\n",
    "        from sklearn.cluster import KMeans\n",
    "        total_predictons = []\n",
    "        for i in range(0,num_iterations):\n",
    "            kmeans = KMeans(n_clusters=self.clusters, random_state=0).fit(self.data)\n",
    "            current_kmeans_predictions = kmeans.labels_\n",
    "            total_predictons.append(pd.DataFrame(current_kmeans_predictions).T)\n",
    "        concatenated = pd.concat(total_predictons)\n",
    "        round_kmeans_predictions = round(concatenated.mean(axis=0))\n",
    "        self.data[\"K_Means_Prediction\"]=round_kmeans_predictions\n",
    "        self.data['K_Means_Prediction']=self.data['K_Means_Prediction'].replace(0,-1)\n",
    "        return data\n",
    "\n",
    "    def isolation_forest(self): \n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        total_predictons = []\n",
    "        for i in range(0,self.iterations):\n",
    "            current_prediction = IsolationForest(random_state=0).fit_predict(self.data)\n",
    "            total_predictons.append(pd.DataFrame(current_prediction).T)\n",
    "        concatenated = pd.concat(total_predictons)\n",
    "        round_predictions = round(concatenated.mean(axis=0))\n",
    "        self.data[\"Isolation_Forest_Prediction\"]=round_predictions\n",
    "        return data\n",
    "    \n",
    "class DIMENSIONALITY_REDUCTION():\n",
    "    def __init__(self,No_of_components):\n",
    "        self.components = No_of_components\n",
    "        self.data = data.copy()\n",
    "    \n",
    "    def GET_pca(self):\n",
    "        pca = PCA(n_components=self.components)\n",
    "        pca.fit(data)\n",
    "        print(\"PCA explains \",round(sum(pca.explained_variance_ratio_),3),\"% of the variance in the data\")\n",
    "        data = pca.fit_transform(self.data)\n",
    "        data = pd.DataFrame(data)\n",
    "        if self.components == 2:\n",
    "            data.columns = [\"PCA1\",\"PCA2\"]\n",
    "        if self.components == 3:\n",
    "            data.columns = [\"PCA1\",\"PCA2\",\"PCA3\"]\n",
    "        return data\n",
    "    \n",
    "    def GET_tsne(self,perplexity=30):\n",
    "        from sklearn.manifold import TSNE\n",
    "        n_components=2 \n",
    "        data =  TSNE(n_components=self.components, \n",
    "                     learning_rate=\"auto\",\n",
    "                     init='random', \n",
    "                     perplexity=perplexity).fit_transform(self.data)\n",
    "        tsne_df = pd.DataFrame(data)\n",
    "        tsne_df.columns = [\"tsne_1\",\"tsne_2\"]\n",
    "        return tsne_df \n",
    "    \n",
    "    def GET_pearson_correlation(self,data):\n",
    "        data = data.dropna()\n",
    "        columns = list(data.columns)\n",
    "        from scipy.stats import pearsonr\n",
    "        result = []\n",
    "        for i in range(0,len(columns)):\n",
    "            for j in range(0,len(columns)):\n",
    "                if i != j and i > j:\n",
    "                    col1,col2 = columns[i],columns[j]\n",
    "                    data1 = data[col1]\n",
    "                    data2 = data[col2]\n",
    "                    corr, _ = pearsonr(data1, data2)\n",
    "                    result.append([col1,col2,corr])\n",
    "                    #print('Pearsons correlation: %.3f' % corr)\n",
    "        result = pd.DataFrame(result)\n",
    "        result.columns = [\"Var1\",\"Var2\",\"Correlation\"]\n",
    "        return result\n",
    "    \n",
    "    def SET_Variable_Selection(self,X,Y,num_of_top_variables):    \n",
    "        xgb = XGBRegressor(n_estimators=100)\n",
    "        xgb.fit(X,Y)\n",
    "        importance = pd.DataFrame([item*100 for item in xgb.feature_importances_])\n",
    "        importance[\"Variable\"]=[item for item in X.columns]\n",
    "        importance.columns = [\"Importance\",\"Var_Name\"]\n",
    "\n",
    "        importance = importance.sort_values(by=[\"Importance\"],ascending = False)\n",
    "        #filtering_out_unncessary_variables = importance[(importance[\"Importance\"] >= 0.1) & (importance[\"Importance\"] <= 1) ]\n",
    "        #importance = importance[importance[\"Var_Name\"] != \"mile_per_kwh\"]\n",
    "        filtering_out_unncessary_variables = importance[0:num_of_top_variables]\n",
    "        filtered_data = X[[item for item in filtering_out_unncessary_variables[\"Var_Name\"]] ]\n",
    "        return filtered_data\n",
    "    \n",
    "    \n",
    "def convert_into_data_format(X_train,X_test):\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    train_X = X_train.reshape((X_train.shape[0],1, X_train.shape[1]))\n",
    "    test_X = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    return train_X, test_X\n",
    "\n",
    "def extract_scaling(data):\n",
    "    feature_extraction_class = Feature_Extraction()\n",
    "    scaled_data, scaling_function = feature_extraction_class.GET_Scaled_data(data)\n",
    "    return scaled_data, scaling_function\n",
    "\n",
    "def get_result(X_train, X_test, Y_scale_fx, y_train, y_test):\n",
    "    xgb_performance = []\n",
    "    xgb = XGBRegressor(n_jobs=5, learning_rate =0.1, subsample=0.8,\n",
    "                         max_depth = 5, min_child_weight = 1, gamma = 0, scale_pos_weight = 1)\n",
    "    model = xgb.fit(X_train,y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    pred = Y_scale_fx.inverse_transform([pred])[0]\n",
    "    xgb_performance.append(mean_absolute_percentage_error(pred,Y_scale_fx.inverse_transform(y_test)))\n",
    "    return np.mean(xgb_performance)\n",
    "\n",
    "def sanity_checks(data):\n",
    "    data = data[data[\"starting_SOC\"] > 0]\n",
    "    data = data[data[\"miles_driven\"] > 0]\n",
    "    data = data[data[\"mile_per_kwh\"] > 0]\n",
    "    data = data[data[\"kwh_per_mile\"] <= 4.5]\n",
    "    data = data[data[\"average_temp\"] <= 110]\n",
    "    data = data.reset_index().drop(columns = [\"index\"])\n",
    "    return data\n",
    "\n",
    "def Data_Cleaning(data, reduction,max_variables, reduction_type,drop_null_values, \n",
    "                  target_variable, \n",
    "                  extra_time_features,\n",
    "                 num_of_components,scaling =False):\n",
    "    print(\"The dataset contains \",data.shape[0],\" rows and \",data.shape[1],\" columns.\")\n",
    "    original_data = data.copy()\n",
    "    variables_to_remove = [\"energy\",\"relative_error\",\n",
    "                        \"end_time\",\n",
    "                        \"max_speed\",\n",
    "                        \"Anomalous\",\n",
    "                        \"accuracy_loss\",\n",
    "                        \"kwh_per_mile\",\n",
    "                        'energy_consuming',\n",
    "                        'energy_regenerating',\n",
    "                        \"average_speed_driven\",\n",
    "                        \"accerlation_max\",\n",
    "                        \"accerlation_avg\",\n",
    "                        \"accerlation_min\",\n",
    "                        \"calculated_rm_mis\",\n",
    "                        \"mile_per_kwh\",\"route\",\n",
    "                        \"predicted_rm_mis\",\n",
    "                        \"accuracy_loss\",\n",
    "                        \"average_humidity\",\n",
    "                        \"estimated\",\n",
    "                        \"route\",\n",
    "                        \"route_id\",\n",
    "                        \"trip_id\",\n",
    "                        \"day_type\",\n",
    "                        \"year\",\n",
    "                        \"mile_per_soc\",\n",
    "                        \"ending_SOC\",\n",
    "                        \"route_id\"]\n",
    "\n",
    "    variables_to_remove = [variable for variable in variables_to_remove if variable != target_variable]\n",
    "    #print(\"\\n\")\n",
    "    #print(\"The following variables have been removed\", variables_to_remove)\n",
    "    #print(\"\\n\")\n",
    "    feature_extraction_class = Feature_Extraction()\n",
    "    data  = feature_extraction_class.GET_deleted_columns(data,variables_to_remove)\n",
    "    print(\"\\n\")\n",
    "    print(\"The extra unncessary columns have been deleted\")\n",
    "    print(\"The dataset now contains \",data.shape[0],\" rows and \",data.shape[1],\" columns.\")\n",
    "    if extra_time_features == True:\n",
    "        data = feature_extraction_class.GET_reduce_time_features(data,\"start_time\")\n",
    "        print(\"\\n\")\n",
    "        print(\"The time related features extracted for the start of the trip\")\n",
    "        print(\"The dataset now contains \",data.shape[0],\" rows and \",data.shape[1],\" columns.\")\n",
    "        \n",
    "    if extra_time_features == False:\n",
    "        print(\"The time related features have not been extracted\")\n",
    "    \n",
    "    if drop_null_values == True:\n",
    "        data = data.dropna().reset_index(inplace = False).drop(columns = [\"index\",\"start_time_year\",\"start_time_month\"])\n",
    "        print(\"\\n\")\n",
    "        print(\"The dataset now contains \",data.shape[0],\" rows and \",data.shape[1],\" columns.\")\n",
    "        print(\"The Null Values have been dropped and index resest with extra column deleted\")\n",
    "\n",
    "        \n",
    "    if drop_null_values == False:\n",
    "        reduction = False\n",
    "        null_columns = data.columns[data.isnull().any()].tolist()\n",
    "        print(\"Null Values are present in\",null_columns)\n",
    "        print(\"Null Values have not been dropped\")\n",
    "        \n",
    "    \n",
    "    data = feature_extraction_class.GET_BUS_DESIGN(data)\n",
    "    if scaling==True:\n",
    "        X_scaled_data,Y_scaled_data = feature_extraction_class.GET_X_and_Y(data,target_variable)\n",
    "\n",
    "        X_data, scaling_function_x = extract_scaling(X_scaled_data)\n",
    "        Y_data, scaling_function_y = extract_scaling(Y_scaled_data)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"The data has been scaled and bus design related features added.\")\n",
    "        print(\"The dataset now contains \",data.shape[0],\" rows and \",data.shape[1],\" columns.\")\n",
    "    if scaling==False:\n",
    "        return data \n",
    "    \n",
    "    if reduction == True:\n",
    "        dimenstionality_reduction_class = DIMENSIONALITY_REDUCTION(No_of_components=3)\n",
    "        X_scaled_data,Y_scaled_data = feature_extraction_class.GET_X_and_Y(data,target_variable)\n",
    "        X_scaled_data = dimenstionality_reduction_class.SET_Variable_Selection(X=X_scaled_data,\n",
    "                                                                               Y=Y_scaled_data,\n",
    "                                                                               num_of_top_variables=max_variables)\n",
    "\n",
    "        if reduction_type == \"pca\":\n",
    "            data = dimenstionality_reduction_class.GET_pca(No_of_components=dimensionality_reduction_dimensions,data=data)\n",
    "        if reduction_type == \"tsne\":\n",
    "            data = dimenstionality_reduction_class.GET_tsne(data)  \n",
    "        return X_scaled_data, Y_scaled_data, X_scale_fx, Y_scale_fx\n",
    "    else:\n",
    "        print(\"\\n\")\n",
    "        print(\"Dimensionality Reduction has been switched off\")\n",
    "        print(\"The data has been returned\")\n",
    "        print(\"\\n\")\n",
    "        print(\"The datset has lost\",original_data.shape[0]-data.shape[0],\" number of rows\")\n",
    "        return X_data, scaling_function_x,Y_data, scaling_function_y\n",
    "    \n",
    "def plot_creations():\n",
    "    variable = \"BATTERY_CAPACITY\"\n",
    "    feature_extraction_class = Feature_Extraction()\n",
    "    original_data = data.copy()\n",
    "    X,Y = feature_extraction_class.GET_X_and_Y(data,target_variable)\n",
    "    X_scaled_data, X_scale_fx = feature_extraction_class.GET_Scaled_data(X)\n",
    "    Y_scaled_data, Y_scale_fx = feature_extraction_class.GET_Scaled_data(Y)\n",
    "    X_scaled_data[target_variable] = Y_scaled_data\n",
    "    data = X_scaled_data\n",
    "    manu_options = list(set(data[variable]))\n",
    "    mapping = {}\n",
    "    for i in range(0,len(data)):\n",
    "        mapping[X_scaled_data[variable][i]] = str(original_data[variable][i])\n",
    "\n",
    "    results = []\n",
    "    feature_extraction_class = Feature_Extraction()\n",
    "    for i in range(0,len(manu_options)):\n",
    "        training_data = data[data[variable]==manu_options[i]]\n",
    "        testing_data = data[data[variable]!=manu_options[i]]\n",
    "        X_train,Y_train = feature_extraction_class.GET_X_and_Y(training_data,target_variable)\n",
    "        X_test,Y_test = feature_extraction_class.GET_X_and_Y(testing_data,target_variable)\n",
    "        results.append([mapping[manu_options[i]],get_result(X_train, X_test,Y_scale_fx, Y_train, Y_test), X_test.shape])\n",
    "        \n",
    "data = pd.read_csv(\"/Users/aasimwani/Downloads/foothill_missing_values_filled - foothill_missing_values_filled.csv\")\n",
    "data[\"start_time\"] = [int(item) for item in data[\"start_time\"]]\n",
    "df2 = pd.read_csv(\"/Users/aasimwani/Downloads/data/culver_city.csv\")\n",
    "data =  data.append(df2)\n",
    "data = sanity_checks(data)\n",
    "data = data.reset_index().drop(columns = [\"index\"])\n",
    "data = data.sort_values(by=[\"trip_id\",\"start_time\"],ascending=True)\n",
    "drop_null_values = True\n",
    "dimensionality_reduction_dimensions = 3\n",
    "max_variables = 10\n",
    "reduction = False # Fix PCA and TSNE\n",
    "extra_time_features = True\n",
    "reduction_type = \"pca\" #or \"tsne\"\n",
    "target_variable = \"kwh_per_mile\"\n",
    "\n",
    "data = Data_Cleaning(data,reduction, max_variables, reduction_type, drop_null_values, target_variable, extra_time_features,\n",
    "                             num_of_components = dimensionality_reduction_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03eb3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3fec6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_processing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X,Y \u001b[38;5;241m=\u001b[39m \u001b[43mdata_processing\u001b[49m\u001b[38;5;241m.\u001b[39mGET_X_and_Y(data2,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwh_per_mile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m data_processing\u001b[38;5;241m.\u001b[39mGET_data_splits(X,Y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_processing' is not defined"
     ]
    }
   ],
   "source": [
    "X,Y = data_processing.GET_X_and_Y(data2,\"kwh_per_mile\")\n",
    "X_train, X_test, y_train, y_test = data_processing.GET_data_splits(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_into_data_format(X_train,X_test):\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    train_X = X_train.reshape((X_train.shape[0],1, X_train.shape[1]))\n",
    "    test_X = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    return train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a568d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X = convert_into_data_format(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc8ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "974f74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8581b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project = \"introduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247cf31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import activations, layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense,LSTM,Embedding, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import BatchNormalization \n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "batch_size = 25\n",
    "epochs = 20\n",
    "learning_rate = 0.15\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.9\n",
    "\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,mode = \"auto\")\n",
    "\n",
    "\"\"\"model = Sequential()\n",
    "model.add(LSTM(200,stateful=True,input_shape=(train_X.shape[1],train_X.shape[2])))\n",
    "model.add(LSTM(200,input_shape=(train_X.shape[1],train_X.shape[2])))\n",
    "model.add(layers.Dense(80, activation='relu'))\n",
    "model.add(Dense(1,activation='tanh'))\"\"\"\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(300, return_sequences=True,input_shape=(train_X.shape[1],train_X.shape[2]))))\n",
    "               #input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(200,return_sequences=True))  # return a single vector of dimension 32\n",
    "model.add(LSTM(200))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(120, activation = \"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(1))\n",
    "    \n",
    "    \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
    "                            decay=decay_rate),loss='mean_absolute_error')\n",
    "\n",
    "\"\"\"model.compile(loss=\"mean_absolute_error\", optimizer=tf.keras.optimizers.Adam(clipnorm=1,\n",
    "                        learning_rate=learning_rate, decay=decay_rate))\"\"\"\n",
    "history  =model.fit(train_X, y_train, batch_size=batch_size, \n",
    "                    callbacks=[WandbCallback()],epochs=epochs, validation_data = (test_X,y_test))\n",
    "\n",
    "\n",
    "pyplot.plot(history.history['loss'], label='Train')\n",
    "pyplot.plot(history.history['val_loss'], label='Validation')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89cbbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f04ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ae089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd117a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb3f543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/aasimwani/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "! wandb login e0b7eb97792ad53473a25d1e2373323e9a2dad3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e89a2775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c53c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fce60115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91b377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8aeca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a662ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "27364d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8bba51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def train_step(self, x, y):\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "def get_model(model, model_params):\n",
    "    models = {\n",
    "        \"rnn\": RNNModel,\n",
    "        \"lstm\": LSTMModel,\n",
    "        \"gru\": GRUModel,\n",
    "    }\n",
    "    return models.get(model.lower())(**model_params)\n",
    "\n",
    "def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n",
    "    model_path = f'models/{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        batch_losses = []\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            loss = self.train_step(x_batch, y_batch)\n",
    "            batch_losses.append(loss)\n",
    "        training_loss = np.mean(batch_losses)\n",
    "        self.train_losses.append(training_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_val_losses = []\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
    "                y_val = y_val.to(device)\n",
    "                self.model.eval()\n",
    "                yhat = self.model(x_val)\n",
    "                val_loss = self.loss_fn(y_val, yhat).item()\n",
    "                batch_val_losses.append(val_loss)\n",
    "            validation_loss = np.mean(batch_val_losses)\n",
    "            self.val_losses.append(validation_loss)\n",
    "\n",
    "        if (epoch <= 10) | (epoch % 50 == 0):\n",
    "            print(\n",
    "                f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "    torch.save(self.model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f5957ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        values = []\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            self.model.eval()\n",
    "            yhat = self.model(x_test)\n",
    "            predictions.append(yhat.to(device).detach().numpy())\n",
    "            values.append(y_test.to(device).detach().numpy())\n",
    "\n",
    "    return predictions, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3ebb79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(self):\n",
    "    plt.plot(self.train_losses, label=\"Training loss\")\n",
    "    plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Losses\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c0660002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "74166508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, h0 = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7b0a5528",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Optimization' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [200]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     24\u001b[0m opt \u001b[38;5;241m=\u001b[39m Optimization(model\u001b[38;5;241m=\u001b[39mmodel, loss_fn\u001b[38;5;241m=\u001b[39mloss_fn, optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m(train_loader, val_loader, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, n_epochs\u001b[38;5;241m=\u001b[39mn_epochs, n_features\u001b[38;5;241m=\u001b[39minput_dim)\n\u001b[1;32m     26\u001b[0m opt\u001b[38;5;241m.\u001b[39mplot_losses()\n\u001b[1;32m     28\u001b[0m predictions, values \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mevaluate(test_loader_one, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_features\u001b[38;5;241m=\u001b[39minput_dim)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Optimization' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_dim = len(X_train.columns)\n",
    "output_dim = 1\n",
    "hidden_dim = 64\n",
    "layer_dim = 3\n",
    "batch_size = 64\n",
    "dropout = 0.2\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "model_params = {'input_dim': input_dim,\n",
    "                'hidden_dim' : hidden_dim,\n",
    "                'layer_dim' : layer_dim,\n",
    "                'output_dim' : output_dim,\n",
    "                'dropout_prob' : dropout}\n",
    "\n",
    "model = get_model('lstm', model_params)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
    "opt.plot_losses()\n",
    "\n",
    "predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7f10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee724b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e3a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae6e89a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6677202b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3866c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
